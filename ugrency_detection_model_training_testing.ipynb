{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pn6XCSVcL6BJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports and Installs"
      ],
      "metadata": {
        "id": "Ysl9S4GQL12L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL5n6KcCCmVI",
        "outputId": "362b0e16-fd59-4391-b3c8-e353b859fc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizerFast\n",
        "import tensorflow as tf\n",
        "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import chardet\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "p4_6ztU6CBVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cWUG57T3XxB",
        "outputId": "07e6192b-ebdb-4e33-9d53-a70dcdb801a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "pn6XCSVcL6BJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hug4VBY7B7pE",
        "outputId": "990e6f2c-83e0-4fc0-fa9a-26625f42229a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "560/560 [==============================] - 648s 1s/step - loss: 0.1708 - accuracy: 0.9405 - val_loss: 0.1277 - val_accuracy: 0.9573\n",
            "187/187 [==============================] - 63s 324ms/step - loss: 0.1376 - accuracy: 0.9511\n",
            "Test loss: 0.137643501162529\n",
            "Test accuracy: 0.9510726928710938\n"
          ]
        }
      ],
      "source": [
        "# SETTING UP TOKENIZER\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# LOADING DATA\n",
        "input_data = pd.read_csv('/content/drive/MyDrive/data_khaled.csv')\n",
        "data = input_data[['Tweet Text','Informativeness']].copy()\n",
        "print(int(3*len(data['Informativeness'].tolist())/4))\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "data = data[0:int(len(data['Informativeness'].tolist())/2)]\n",
        "data.loc[data['Informativeness'] == 'informative', 'Informativeness'] = 1\n",
        "data.loc[data['Informativeness'] == 'not_informative', 'Informativeness'] = 0\n",
        "\n",
        "# SPLITTING DATA INTO TRAINING, VALIDATION AND TESTING\n",
        "# Train: 60%, Validation: 20%, Test: 20%\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(data['Tweet Text'], data['Informativeness'], test_size=0.2) \n",
        "xtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.25)\n",
        "\n",
        "# ENCODING DATA\n",
        "train_encodings = tokenizer(xtrain.tolist(), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(xval.tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(xtest.tolist(), truncation=True, padding=True)\n",
        "\n",
        "# LOADING DATA INTO HUGGINGFACE DATASET OBJECT\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), ytrain.tolist()))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), yval.tolist()))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), ytest.tolist()))\n",
        "\n",
        "# TRAINING MODEL\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 90\n",
        "NUM_TRAIN_BATCHES = int(math.ceil(len(train_dataset) / BATCH_SIZE))\n",
        "NUM_VAL_BATCHES = int(math.ceil(len(val_dataset) / BATCH_SIZE))\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset.shuffle(len(train_dataset)).batch(BATCH_SIZE),\n",
        "    validation_data=val_dataset.batch(BATCH_SIZE),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    steps_per_epoch=NUM_TRAIN_BATCHES,\n",
        "    validation_steps=NUM_VAL_BATCHES\n",
        ")\n",
        "\n",
        "# TESTING MODEL\n",
        "test_loss, test_acc = model.evaluate(test_dataset.batch(BATCH_SIZE))\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving Model"
      ],
      "metadata": {
        "id": "FoP_Xi0oNZEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/saved_model')"
      ],
      "metadata": {
        "id": "YqEOFm8_cH3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "1AzvtmlPAf2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Data and extracting tweets column only"
      ],
      "metadata": {
        "id": "Qd21pWKwMP18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/tweet_dicts.json') as f:\n",
        "    raw_data = f.read()\n",
        "\n",
        "test_data = json.loads(raw_data)\n",
        "\n",
        "input_data = pd.DataFrame(test_data)\n",
        "test_tweets = input_data['text'].tolist()"
      ],
      "metadata": {
        "id": "dUf6IJADlTAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "PBCTFxTcMpTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding Data\n",
        "real_data_encodings = tokenizer(test_tweets, truncation=True, padding=True)\n",
        "# Converting Data to a Huggingface Dataset object\n",
        "dataset = tf.data.Dataset.from_tensor_slices(dict(real_data_encodings))\n",
        "#Prediction\n",
        "predictions = model.predict(dataset.batch(BATCH_SIZE))\n",
        "predicted_labels = np.argmax(predictions.to_tuple()[0], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGP_cHlyB5sr",
        "outputId": "5a8e61e8-78e3-4f3e-cfb5-6c895eaf9237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 115ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a dataframe of the detected tweets and their labels\n",
        "d = {'Tweets':test_tweets, 'Label': predicted_labels.tolist() }\n",
        "results = pd.DataFrame(d)"
      ],
      "metadata": {
        "id": "YEQzCtxwEZ_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[results['Label']==1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "TmL5CDhEErzj",
        "outputId": "25fe2f7a-c350-4689-e757-3647bd46fa28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Tweets  Label\n",
              "9    Iran is doubling down on military support for ...      1\n",
              "20   Following Donald Trump – Hungary will Move its...      1\n",
              "31   Warning that energy crisis could ‘collapse’ Du...      1\n",
              "45   After fleeing war, homelessness threatens UK’s...      1\n",
              "50   BREAKING NEWS FOR NO BRAINS COCK SUCKER PAK PM...      1\n",
              "62   Honduran illegal immigrant who pretended to be...      1\n",
              "66   Warning that energy crisis could ‘collapse’ Du...      1\n",
              "68   From prison to the frontlines: Thousands of Ru...      1\n",
              "71   Deputy gangs a 'cancer' within the Los Angeles...      1\n",
              "74   War in Ukraine 03 03 2023a https://t.co/2UYgue...      1\n",
              "82   National Safety Day\\n#safety #safetyfirst #cov...      1\n",
              "91   Binance launches anti-scam campaign after Hong...      1\n",
              "97   ‘A historic first’: Vietnam allows Franklin Gr...      1\n",
              "102  Frontline Videos Show Intense Battles In Easte...      1\n",
              "107  If there is an emergency out in SPACE; then it...      1\n",
              "108  Who’d a guessed China might be the be ones to ...      1\n",
              "113  Health companies gave generously to President ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92b53474-fd46-422c-b5ec-89253e3f3039\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweets</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Iran is doubling down on military support for ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Following Donald Trump – Hungary will Move its...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Warning that energy crisis could ‘collapse’ Du...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>After fleeing war, homelessness threatens UK’s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>BREAKING NEWS FOR NO BRAINS COCK SUCKER PAK PM...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>Honduran illegal immigrant who pretended to be...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>Warning that energy crisis could ‘collapse’ Du...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>From prison to the frontlines: Thousands of Ru...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>Deputy gangs a 'cancer' within the Los Angeles...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>War in Ukraine 03 03 2023a https://t.co/2UYgue...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>National Safety Day\\n#safety #safetyfirst #cov...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>Binance launches anti-scam campaign after Hong...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>‘A historic first’: Vietnam allows Franklin Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>Frontline Videos Show Intense Battles In Easte...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>If there is an emergency out in SPACE; then it...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>Who’d a guessed China might be the be ones to ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Health companies gave generously to President ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92b53474-fd46-422c-b5ec-89253e3f3039')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92b53474-fd46-422c-b5ec-89253e3f3039 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92b53474-fd46-422c-b5ec-89253e3f3039');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.iloc[108]['Tweets']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "LR8wyQWjNo3K",
        "outputId": "78affbb5-2155-4f3c-ab9a-fd18ba236577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Who’d a guessed China might be the be ones to broker peace between Ukraine &amp;Russia? \\n\\nProbably driven more by the drop in productivity &amp; a global recession causing sales of their export goods to drop and their economy  to slow than some other altruistic motive, but I’ll take it. https://t.co/3UqGiR2rOx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving Model to Drive"
      ],
      "metadata": {
        "id": "5d82IjNUI4Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/my_model_weights.h5' /content/drive/MyDrive/Bin_Backup/my_model_weights.h5"
      ],
      "metadata": {
        "id": "XsyTLFVDIxOg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}